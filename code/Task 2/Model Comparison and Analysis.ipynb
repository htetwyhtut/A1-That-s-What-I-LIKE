{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from class_function import Skipgram, SkipgramNeg, Glove\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training data\n",
    "Data = pickle.load(open('../../models/Data.pkl', 'rb'))\n",
    "\n",
    "corpus = Data['corpus']\n",
    "vocab = Data['vocab']\n",
    "word2index = Data['word2index']\n",
    "voc_size = Data['voc_size']\n",
    "embed_size = Data['embedding_size']\n",
    "window_size = Data['window_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['embedding_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['voc_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_21096\\2717541855.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  skipgram.load_state_dict(torch.load('../../models/Word2Vec(Skipgram).pt'),  strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(7136, 50)\n",
       "  (embedding_outside): Embedding(7136, 50)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the saved Skipgram model\n",
    "\n",
    "skipgram = Skipgram(voc_size, embed_size)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "skipgram.load_state_dict(torch.load('../../models/Word2Vec(Skipgram).pt'),  strict=False)\n",
    "\n",
    "skipgram.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_21096\\2530059108.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('../../models/Word2Vec(Neg_Sampling).pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipgramNeg(\n",
       "  (embedding_center): Embedding(7136, 50)\n",
       "  (embedding_outside): Embedding(7136, 50)\n",
       "  (logsigmoid): LogSigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the saved negative Skipgram model\n",
    "state_dict = torch.load('../../models/Word2Vec(Neg_Sampling).pt')\n",
    "\n",
    "# Load the remapped state_dict into the model\n",
    "skipgramNeg = SkipgramNeg(voc_size, embed_size)\n",
    "\n",
    "skipgramNeg.load_state_dict(state_dict)\n",
    "\n",
    "skipgramNeg.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqih\\AppData\\Local\\Temp\\ipykernel_21096\\1640742066.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  glove.load_state_dict(torch.load('../../models/Glove_from_scratch.pt'),  strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Glove(\n",
       "  (center_embedding): Embedding(7136, 50)\n",
       "  (outside_embedding): Embedding(7136, 50)\n",
       "  (center_bias): Embedding(7136, 1)\n",
       "  (outside_bias): Embedding(7136, 1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the saved Glove from scratch model\n",
    "\n",
    "glove = Glove(voc_size, embed_size)\n",
    "\n",
    "glove.load_state_dict(torch.load('../../models/Glove_from_scratch.pt'),  strict=False)\n",
    "\n",
    "glove.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Glove (Gensim) like example from Dr. Chaklam\n",
    "glove_file = datapath(r'C:\\Users\\zaqih\\Downloads\\glove.6B\\glove.6B.100d.txt')\n",
    "\n",
    "gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function to compute vectors for all words in the vocabulary\n",
    "def compute_all_word_vectors(vocab, model):\n",
    "    word_vectors = []\n",
    "    for word in vocab:\n",
    "        word_vectors.append(model.get_vector(word))\n",
    "    return torch.stack(word_vectors)\n",
    "\n",
    "# Custom Function to caluates the performance of a word embedding model and return accuracy\n",
    "def similarities(lines, model, vocab):\n",
    "    all_word_vectors = compute_all_word_vectors(vocab, model)\n",
    "\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "\n",
    "        # Skip lines with unknown words\n",
    "        if any(word not in vocab for word in words):\n",
    "            skipped += 1\n",
    "            # print(f\"Skipping analogy due to unknown words: {line}\")\n",
    "            continue\n",
    "\n",
    "        # Retrieve vectors for analogy words\n",
    "        vectors = [model.get_vector(word.lower()) for word in words]\n",
    "\n",
    "        # Perform vector manipulation\n",
    "        result_vector = vectors[1] - vectors[0] + vectors[2]\n",
    "        result_vector = result_vector.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = F.cosine_similarity(result_vector, all_word_vectors)\n",
    "\n",
    "        # Get the closest word\n",
    "        closest_word_index = torch.argmax(similarities).item()\n",
    "        closest_word = vocab[closest_word_index]\n",
    "\n",
    "        if closest_word == words[3]:  # Check if predicted word matches target\n",
    "            correct += 1\n",
    "        # else:\n",
    "        #     # print(f\"Mismatch: {line} -> Predicted: {closest_word}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    total = len(lines) - skipped\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    # print('---------------------------------------------------------')\n",
    "    # print(f'Total: {total} analogies')\n",
    "    # print(f'Skipped: {skipped} analogies (unknown words)')\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function to evaluate Gensim\n",
    "\n",
    "\n",
    "def evaluate_glove(lines, model):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.lower().split()  # Convert line to lowercase and split into words\n",
    "\n",
    "        # Check if line is valid and all words exist in the model\n",
    "        if len(words) != 4:\n",
    "            print(f\"Skipping malformed line: {line}\")\n",
    "            continue\n",
    "        if any(word not in model for word in words):\n",
    "            print(f\"Skipping line due to OOV words: {line}\")\n",
    "            continue\n",
    "\n",
    "        # Perform analogy\n",
    "        try:\n",
    "            result = model.most_similar(positive=[words[2], words[1]], negative=[words[0]], topn=1)\n",
    "            closest_word = result[0][0]  # Get the most similar word\n",
    "            total += 1\n",
    "\n",
    "            if closest_word == words[3]:\n",
    "                correct += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line {line}: {e}\")\n",
    "            continue\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    # print('---------------------------------------------------------')\n",
    "    # print(f'Total lines evaluated: {total}')\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and return cosine similarity between two vectors\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "\n",
    "    # Flatten vectors to ensure they're 1D\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "\n",
    "    # Compute dot product and norms\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "\n",
    "    # Return cosine similarity\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate word similarity and Return the Spearman rank correlation \n",
    "\n",
    "def similar(lines, model):\n",
    "    \n",
    "    scores_real = []  # To store actual similarity scores (from the dataset)\n",
    "    scores_pred = []  # To store predicted similarity scores (using cosine similarity)\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()  # Split line into words\n",
    "        vec = []  # List to store word vectors\n",
    "\n",
    "        # Assuming the first two words need to be compared\n",
    "        for word in words[:2]: \n",
    "            try:\n",
    "                # Attempt to get the vector for the word\n",
    "                vec.append(model.get_vector(word).detach().numpy())\n",
    "            except:\n",
    "                # If the word is not in the vocabulary, use the <UNK> token\n",
    "                vec.append(model.get_vector('<UNK>').detach().numpy())\n",
    "\n",
    "        # Store the actual similarity score from the dataset (third word)\n",
    "        scores_real.append(float(words[2]))  \n",
    "        \n",
    "        # Calculate the cosine similarity between the two words and store the predicted score\n",
    "        scores_pred.append(cosine_similarity(np.array(vec[0]), np.array(vec[1])))\n",
    "\n",
    "    # Calculate and return Spearman's rank correlation between actual and predicted scores\n",
    "    return spearmanr(scores_real, scores_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function to Evaluate word similarity and Return the Spearman rank correlation for Gensim\n",
    "\n",
    "def similar_gensim(lines, model):\n",
    "    scores_real = []  # Store real human similarity scores\n",
    "    scores_pred = []  # Store predicted cosine similarities based on embeddings\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()  # Split each line into words\n",
    "        vec = []\n",
    "        \n",
    "        # Extract word vectors for the first two words\n",
    "        for word in words[:2]:\n",
    "            try:\n",
    "                # Use model[word] to get the embedding directly from the model (Gensim)\n",
    "                vec.append(model[word])\n",
    "            except KeyError:\n",
    "                # Handle missing words by using a placeholder or a default embedding\n",
    "                # Here I use 'unk' as an example. Adjust based on your vocabulary.\n",
    "                vec.append(model['unk'])  # You can use your own word for unknown words.\n",
    "        \n",
    "        # Append human similarity score (the third element of each line)\n",
    "        scores_real.append(float(words[2]))\n",
    "\n",
    "        # Compute the predicted similarity using cosine similarity\n",
    "        similarity_score = cosine_similarity(np.array(vec[0]), np.array(vec[1]))\n",
    "        scores_pred.append(similarity_score)\n",
    "\n",
    "    # Calculate Spearman's rank correlation between real and predicted scores\n",
    "    correlation, p_value = spearmanr(scores_real, scores_pred)\n",
    "\n",
    "    # print(f\"Spearman Rank Correlation of Gensim: {correlation:.4f}\")\n",
    "    # print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    return correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Compute cosine similarity between two words using a given model.\n",
    "# def compute_similarity(word1, word2, model):\n",
    "#     try:\n",
    "#         # Handle Gensim-like models (dictionary-like objects)\n",
    "#         if isinstance(model, dict) or hasattr(model, '__getitem__'):\n",
    "#             vec1 = model[word1].reshape(1, -1)  # Get vector for word1\n",
    "#             vec2 = model[word2].reshape(1, -1)  # Get vector for word2\n",
    "#         # Handle PyTorch models\n",
    "#         elif hasattr(model, 'get_vector'):\n",
    "#             vec1 = model.get_vector(word1).detach().numpy().reshape(1, -1)\n",
    "#             vec2 = model.get_vector(word2).detach().numpy().reshape(1, -1)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported model type\")\n",
    "        \n",
    "#         # Compute cosine similarity\n",
    "#         return cosine_similarity(vec1, vec2)[0][0]\n",
    "#     except KeyError:\n",
    "#         # Handle unknown words (e.g., return 0 or use a default vector)\n",
    "#         return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(word1, word2, model):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two words using a given model.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): First word.\n",
    "        word2 (str): Second word.\n",
    "        model: A word embedding model (either PyTorch-based, Gensim-based, or a dictionary-like object).\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine similarity between the two word vectors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle Gensim-like models (dictionary-like objects)\n",
    "        if isinstance(model, dict) or hasattr(model, '__getitem__'):\n",
    "            vec1 = np.array(model[word1]).reshape(1, -1)  # Get vector for word1 and reshape to 2D\n",
    "            vec2 = np.array(model[word2]).reshape(1, -1)  # Get vector for word2 and reshape to 2D\n",
    "        # Handle PyTorch models\n",
    "        elif hasattr(model, 'get_vector'):\n",
    "            vec1 = model.get_vector(word1).detach().numpy().reshape(1, -1)  # Get vector for word1 and reshape to 2D\n",
    "            vec2 = model.get_vector(word2).detach().numpy().reshape(1, -1)  # Get vector for word2 and reshape to 2D\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type\")\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        return cosine_similarity(vec1, vec2)  # No need to index [0][0] since it returns a scalar\n",
    "    except KeyError:\n",
    "        # Handle unknown words (e.g., return 0 or use a default vector)\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['// Copyright 2013 Google Inc. All Rights Reserved.',\n",
       " ': capital-common-countries',\n",
       " 'Athens Greece Baghdad Iraq',\n",
       " 'Athens Greece Bangkok Thailand',\n",
       " 'Athens Greece Beijing China',\n",
       " 'Athens Greece Berlin Germany',\n",
       " 'Athens Greece Bern Switzerland']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the dataset for testing\n",
    "file_path = \"../../data/word-test.v1.txt\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    contents = f.read()\n",
    "    data = contents.split('\\n')\n",
    "\n",
    "data[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : capital-common-countries\n",
      "508 : capital-world\n",
      "5033 : currency\n",
      "5900 : city-in-state\n",
      "8368 : family\n",
      "8875 : gram1-adjective-to-adverb\n",
      "9868 : gram2-opposite\n",
      "10681 : gram3-comparative\n",
      "12014 : gram4-superlative\n",
      "13137 : gram5-present-participle\n",
      "14194 : gram6-nationality-adjective\n",
      "15794 : gram7-past-tense\n",
      "17355 : gram8-plural\n",
      "18688 : gram9-plural-verbs\n",
      "Number of empty strings in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "#explore the dataset\n",
    "empty_count = 0\n",
    "for idx, sent in enumerate(data):\n",
    "    if not sent:  # Check if the string is empty\n",
    "        empty_count += 1\n",
    "    elif sent[0] == ':':\n",
    "        print(idx, sent)\n",
    "\n",
    "print(f\"Number of empty strings in the dataset: {empty_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Athens Greece Baghdad Iraq', 'Athens Greece Bangkok Thailand', 'Athens Greece Beijing China', 'Athens Greece Berlin Germany', 'Athens Greece Bern Switzerland']\n",
      "['dancing danced decreasing decreased', 'dancing danced describing described', 'dancing danced enhancing enhanced', 'dancing danced falling fell', 'dancing danced feeding fed']\n"
     ]
    }
   ],
   "source": [
    "#create the corpora for testing\n",
    "\n",
    "# capital-common-countries corpus to be used for semantic analogies as per assignment\n",
    "capital = data[2:508]\n",
    "sem_lines = capital\n",
    "print(sem_lines[:5])\n",
    "\n",
    "# past-tense corpus to be used for syntatic analogies\n",
    "past = data[15795:17355]\n",
    "syn_lines = past\n",
    "print(syn_lines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and their names\n",
    "models = {\n",
    "    'Word2Vec (Skipgram)': skipgram,\n",
    "    'Word2Vec (Neg Sampling)': skipgramNeg,\n",
    "    'GloVe from Scratch': glove,\n",
    "    'GloVe (Gensim)': gensim\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Analysis:\n",
      "\n",
      "Evaluating Word2Vec (Skipgram) on semantic analogies:\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Evaluating Word2Vec (Neg Sampling) on semantic analogies:\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Evaluating GloVe from Scratch on semantic analogies:\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Evaluating GloVe (Gensim) on semantic analogies:\n",
      "Accuracy: 93.87%\n"
     ]
    }
   ],
   "source": [
    "# Perform semantic analysis\n",
    "print(\"Semantic Analysis:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} on semantic analogies:\")\n",
    "    if model_name == 'GloVe (Gensim)':\n",
    "        # Use evaluate_glove for gensim model\n",
    "        evaluate_glove(sem_lines, model)\n",
    "    else:\n",
    "        # Use similarities for other models\n",
    "        similarities(sem_lines, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntatic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Syntactic Analysis:\n",
      "\n",
      "Evaluating Word2Vec (Skipgram) on syntactic analogies:\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Evaluating Word2Vec (Neg Sampling) on syntactic analogies:\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Evaluating GloVe from Scratch on syntactic analogies:\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Evaluating GloVe (Gensim) on syntactic analogies:\n",
      "Accuracy: 55.45%\n"
     ]
    }
   ],
   "source": [
    "# Perform syntactic analysis\n",
    "print(\"\\nSyntactic Analysis:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} on syntactic analogies:\")\n",
    "    if model_name == 'GloVe (Gensim)':\n",
    "        # Use evaluate_glove for gensim model\n",
    "        evaluate_glove(syn_lines, model)\n",
    "    else:\n",
    "        # Use similarities for other models\n",
    "        similarities(syn_lines, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<compariosn of models.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the similarity dataset to find correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the similarity dataset for testing\n",
    "file_path = \"../../data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    similarity_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and their evaluation functions\n",
    "models = {\n",
    "    'Word2Vec (Skipgram)': (skipgram, similar),\n",
    "    'Word2Vec (Neg Sampling)': (skipgramNeg, similar),\n",
    "    'GloVe from Scratch': (glove, similar),\n",
    "    'GloVe (Gensim)': (gensim, similar_gensim)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec (Skipgram) correlation score: 0.1635\n",
      "Word2Vec (Neg Sampling) correlation score: 0.1589\n",
      "GloVe from Scratch correlation score: 0.1757\n",
      "GloVe (Gensim) correlation score: 0.6038\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model\n",
    "for model_name, (model, eval_func) in models.items():\n",
    "    if model_name == 'gensim':\n",
    "        # Use the specific evaluation function for Gensim\n",
    "        correlation_score = eval_func(similarity_lines, model)[0]\n",
    "    else:\n",
    "        # Use the general evaluation function for other models\n",
    "        correlation_score = eval_func(similarity_lines, model)[0]\n",
    "    \n",
    "    # Print the correlation score\n",
    "    print(f'{model_name} correlation score: {correlation_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation scores between the models' predicted similarity scores (dot product) and human-annotated similarity scores are as follows:\n",
    "\n",
    "Word2Vec (Skipgram): 0.1635 (weak correlation)\n",
    "\n",
    "Word2Vec (Neg Sampling): 0.1589 (weak correlation)\n",
    "\n",
    "GloVe from Scratch: 0.1757 (weak correlation)\n",
    "\n",
    "GloVe (Gensim): 0.6038 (moderate to strong correlation)\n",
    "\n",
    "**Conclusion:** The GloVe (Gensim) pre-trained model demonstrates the strongest correlation with human judgments, indicating that its embeddings align well with human understanding of word similarity. \n",
    "\n",
    "In contrast, the Word2Vec models and the GloVe model trained from scratch show weak correlations, suggesting that their embeddings do not capture semantic relationships as effectively. \n",
    "\n",
    "This highlights the importance of using well-trained, high-quality embeddings for tasks requiring alignment with human judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2\n",
       "0       tiger    cat   7.35\n",
       "1       tiger  tiger  10.00\n",
       "2       plane    car   5.77\n",
       "3       train    car   6.31\n",
       "4  television  radio   6.77"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load similarity data in table format\n",
    "df = pd.read_table(file_path, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features and target\n",
    "x1 = df.iloc[:, 0]\n",
    "x2 = df.iloc[:, 1]\n",
    "y  = df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted similarity scores for each model\n",
    "y_pred_skipgram = [compute_similarity(w1, w2, skipgram) for w1, w2 in zip(x1, x2)]\n",
    "y_pred_neg = [compute_similarity(w1, w2, skipgramNeg) for w1, w2 in zip(x1, x2)]\n",
    "y_pred_glove = [compute_similarity(w1, w2, glove) for w1, w2 in zip(x1, x2)]\n",
    "y_pred_glove_gensim = [compute_similarity(w1, w2, gensim) for w1, w2 in zip(x1, x2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE for each model\n",
    "mse_skipgram = mean_squared_error(y, y_pred_skipgram)\n",
    "mse_neg = mean_squared_error(y, y_pred_neg)\n",
    "mse_glove = mean_squared_error(y, y_pred_glove)\n",
    "mse_glove_gensim = mean_squared_error(y, y_pred_glove_gensim)\n",
    "mse_y_true = mean_squared_error(y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Skipgram): 32.6069\n",
      "MSE (NEG): 32.5223\n",
      "MSE (GloVe): 32.6296\n",
      "MSE (GloVe Gensim): 27.8081\n",
      "MSE (Y true): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f'MSE (Skipgram): {mse_skipgram:.4f}')\n",
    "print(f'MSE (NEG): {mse_neg:.4f}')\n",
    "print(f'MSE (GloVe): {mse_glove:.4f}')\n",
    "print(f'MSE (GloVe Gensim): {mse_glove_gensim:.4f}')\n",
    "print(f'MSE (Y true): {mse_y_true:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<comparison of mse.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe (Gensim) model performs the best among the four models, with the lowest MSE (27.8081).\n",
    "\n",
    "However, all models show relatively similar MSE values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
